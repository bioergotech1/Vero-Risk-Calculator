{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d009439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial rows: 406\n",
      "Unique patients: 406\n",
      "\n",
      "After merge:\n",
      "Rows: 408\n",
      "Unique patients: 406\n",
      "\n",
      "After de-duplication:\n",
      "Rows: 406\n",
      "Unique patients: 406\n",
      "\n",
      "Saved clean clustering matrix to:\n",
      "C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phase3_clustering_matrix_with_fi.csv\n",
      "\n",
      "Preview:\n",
      "                           patient_id  z_risk_death  z_risk_hosp  z_risk_adr  \\\n",
      "0                       10_AO San Pio     -0.276435     0.928405         0.0   \n",
      "1               10_AORN A. Cardarelli      0.073192     0.556028         0.0   \n",
      "2  10_AORN Monaldi – Cotugno - C.T.O.      1.891840     0.282366         0.0   \n",
      "3        10_AORN San Giuseppe Moscati     -0.356087     1.597014         0.0   \n",
      "4  10_AORN Sant’Anna e San Sebastiano     -0.337761    -0.222885         0.0   \n",
      "\n",
      "   z_fi_lab  \n",
      "0  1.196562  \n",
      "1  1.196562  \n",
      "2  2.170635  \n",
      "3 -0.264548  \n",
      "4 -0.134672  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "PHASE3_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Load clustering matrix\n",
    "# -----------------------------\n",
    "X = pd.read_csv(os.path.join(PHASE3_DIR, \"phase3_clustering_matrix.csv\"))\n",
    "X.columns = [str(c).strip() for c in X.columns]\n",
    "\n",
    "# Basic sanity check\n",
    "assert \"patient_id\" in X.columns, \"patient_id column missing\"\n",
    "X[\"patient_id\"] = X[\"patient_id\"].astype(str).str.strip()\n",
    "\n",
    "print(\"Initial rows:\", len(X))\n",
    "print(\"Unique patients:\", X[\"patient_id\"].nunique())\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Load FI-LAB\n",
    "# -----------------------------\n",
    "fi = pd.read_excel(r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\FI_lab_score.xlsx\")\n",
    "fi.columns = [str(c).strip() for c in fi.columns]\n",
    "fi[\"patient_id\"] = fi[\"patient_id\"].astype(str).str.strip()\n",
    "\n",
    "# Auto-detect FI column\n",
    "fi_candidates = [c for c in fi.columns if c.lower() in [\"fi_lab\", \"fi_lab_score\", \"fi_score\", \"fi\"]]\n",
    "if not fi_candidates:\n",
    "    raise ValueError(f\"Could not detect FI column. Columns: {fi.columns.tolist()}\")\n",
    "\n",
    "FI_COL = fi_candidates[0]\n",
    "\n",
    "fi = fi[[\"patient_id\", FI_COL]].rename(columns={FI_COL: \"FI_LAB\"})\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Merge FI once\n",
    "# -----------------------------\n",
    "X2 = X.merge(fi, on=\"patient_id\", how=\"left\")\n",
    "\n",
    "# Coerce FI to numeric and median-impute\n",
    "X2[\"FI_LAB\"] = pd.to_numeric(X2[\"FI_LAB\"], errors=\"coerce\")\n",
    "X2[\"FI_LAB\"] = X2[\"FI_LAB\"].fillna(X2[\"FI_LAB\"].median())\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Resolve duplicate patients (critical fix)\n",
    "# -----------------------------\n",
    "print(\"\\nAfter merge:\")\n",
    "print(\"Rows:\", len(X2))\n",
    "print(\"Unique patients:\", X2[\"patient_id\"].nunique())\n",
    "\n",
    "# Collapse to one row per patient\n",
    "# Risk scores are identical by design; FI may differ slightly\n",
    "X2_clean = (\n",
    "    X2\n",
    "    .groupby(\"patient_id\", as_index=False)\n",
    "    .agg({\n",
    "        \"z_risk_death\": \"first\",\n",
    "        \"z_risk_hosp\": \"first\",\n",
    "        \"z_risk_adr\": \"first\",\n",
    "        \"FI_LAB\": \"mean\"\n",
    "    })\n",
    ")\n",
    "\n",
    "print(\"\\nAfter de-duplication:\")\n",
    "print(\"Rows:\", len(X2_clean))\n",
    "print(\"Unique patients:\", X2_clean[\"patient_id\"].nunique())\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Z-scale FI-LAB\n",
    "# -----------------------------\n",
    "scaler = StandardScaler()\n",
    "X2_clean[\"z_fi_lab\"] = scaler.fit_transform(X2_clean[[\"FI_LAB\"]])\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Final clustering matrix\n",
    "# -----------------------------\n",
    "X2_out = X2_clean[\n",
    "    [\"patient_id\", \"z_risk_death\", \"z_risk_hosp\", \"z_risk_adr\", \"z_fi_lab\"]\n",
    "].copy()\n",
    "\n",
    "out_path = os.path.join(PHASE3_DIR, \"phase3_clustering_matrix_with_fi.csv\")\n",
    "X2_out.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\nSaved clean clustering matrix to:\")\n",
    "print(out_path)\n",
    "print(\"\\nPreview:\")\n",
    "print(X2_out.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d039989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 3 PCA-then-KMeans phenotyping complete.\n",
      "Phenotype counts: {0: 272, 1: 134}\n",
      "Cohen's d (PC1): 2.07\n",
      "\n",
      "Saved CSVs:\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\phenotype_labels_clean.csv\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\phenotype_summary.csv\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\phenotype_pca_scores.csv\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\phenotype_frequency_counts.csv\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\phenotyping_runlog.txt\n",
      "\n",
      "Saved plots:\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\pca_scatter_pc1_pc2_by_phenotype.png\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\cluster_size_distribution.png\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\pca_variance_explained.png\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\\tsne_2d_by_phenotype.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Optional (nicer scatter)\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    HAS_SEABORN = True\n",
    "except Exception:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "# Optional t-SNE plot\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    HAS_TSNE = True\n",
    "except Exception:\n",
    "    HAS_TSNE = False\n",
    "\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"[<>:\\\"/\\\\|?*]\", \"_\", s)  # Windows invalid chars\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def phase3_pca_then_kmeans_phenotyping_with_plots(\n",
    "    clustering_csv: str,\n",
    "    out_dir: str,\n",
    "    id_col: str = \"patient_id\",\n",
    "    feature_cols=None,\n",
    "    standardize: bool = True,\n",
    "    k: int = 2,\n",
    "    pca_var: float = 0.90,          # KEEP PCs to explain this variance (key change)\n",
    "    n_init: int = 100,              # slightly stronger than 50\n",
    "    random_state: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Phase 3: PCA -> KMeans (k=2) phenotyping + plots + outputs.\n",
    "\n",
    "    Minimal change from your KMeans version:\n",
    "      - Standardize features\n",
    "      - PCA reduction (retain pca_var variance)\n",
    "      - KMeans runs on PCA space (NOT full feature space)\n",
    "      - Silhouette computed in PCA space\n",
    "      - Still produce PCA scatter, tSNE plot (visual), distribution, summaries\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Load + clean\n",
    "    # ----------------------------\n",
    "    df = pd.read_csv(clustering_csv)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\"z_risk_death\", \"z_risk_hosp\", \"z_risk_adr\", \"z_fi_lab\"]\n",
    "\n",
    "    missing = [c for c in feature_cols + [id_col] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    df[id_col] = df[id_col].astype(str).str.strip()\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.sort_values(id_col).drop_duplicates(subset=[id_col], keep=\"first\").copy()\n",
    "    after = len(df)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Numeric safety + impute\n",
    "    # ----------------------------\n",
    "    X = df[[id_col] + feature_cols].copy()\n",
    "    for c in feature_cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "        X[c] = X[c].fillna(X[c].median())\n",
    "\n",
    "    Z = X[feature_cols].values\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Standardize\n",
    "    # ----------------------------\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        Zs = scaler.fit_transform(Z)\n",
    "    else:\n",
    "        scaler = None\n",
    "        Zs = Z\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4) PCA (KEY CHANGE)\n",
    "    # ----------------------------\n",
    "    pca = PCA(n_components=pca_var, random_state=random_state)\n",
    "    Z_pca = pca.fit_transform(Zs)\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "    # Keep first 2 PCs for plotting\n",
    "    pc1 = Z_pca[:, 0]\n",
    "    pc2 = Z_pca[:, 1] if Z_pca.shape[1] > 1 else np.zeros_like(pc1)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5) KMeans on PCA space (KEY CHANGE)\n",
    "    # ----------------------------\n",
    "    km = KMeans(n_clusters=k, random_state=random_state, n_init=n_init)\n",
    "    labels = km.fit_predict(Z_pca)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6) Metrics\n",
    "    # ----------------------------\n",
    "    sil = silhouette_score(Z_pca, labels) if len(np.unique(labels)) > 1 else np.nan\n",
    "\n",
    "    # Cohen's d on PC1 (still useful as separation effect size)\n",
    "    pc1_0 = pc1[labels == 0]\n",
    "    pc1_1 = pc1[labels == 1]\n",
    "    pooled_sd = np.sqrt(((pc1_0.var(ddof=1) + pc1_1.var(ddof=1)) / 2))\n",
    "    cohens_d = (pc1_1.mean() - pc1_0.mean()) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "    labels_df = pd.DataFrame({id_col: X[id_col].values, \"phenotype\": labels})\n",
    "\n",
    "    # Frequency counts\n",
    "    freq = labels_df[\"phenotype\"].value_counts().sort_index()\n",
    "    freq_df = pd.DataFrame({\n",
    "        \"phenotype\": freq.index,\n",
    "        \"count\": freq.values,\n",
    "        \"percent\": (freq.values / freq.values.sum() * 100.0)\n",
    "    })\n",
    "\n",
    "    # ----------------------------\n",
    "    # 7) Summaries (by phenotype)\n",
    "    # ----------------------------\n",
    "    summary = (\n",
    "        pd.concat([X[[id_col] + feature_cols], labels_df[\"phenotype\"]], axis=1)\n",
    "        .groupby(\"phenotype\")[feature_cols]\n",
    "        .agg([\"mean\", \"std\", \"median\", \"min\", \"max\"])\n",
    "    )\n",
    "    summary.columns = [\"_\".join(map(str, c)) for c in summary.columns]\n",
    "    summary.reset_index(inplace=True)\n",
    "\n",
    "    # PCA scores export (first 3 PCs if available)\n",
    "    n_pc_export = min(3, Z_pca.shape[1])\n",
    "    pca_scores = pd.DataFrame(Z_pca[:, :n_pc_export], columns=[f\"PC{i+1}\" for i in range(n_pc_export)])\n",
    "    pca_scores.insert(0, id_col, X[id_col].values)\n",
    "    pca_scores[\"phenotype\"] = labels\n",
    "\n",
    "    # ----------------------------\n",
    "    # 8) Save outputs\n",
    "    # ----------------------------\n",
    "    labels_path = os.path.join(out_dir, \"phenotype_labels_clean.csv\")\n",
    "    summary_path = os.path.join(out_dir, \"phenotype_summary.csv\")\n",
    "    pca_path = os.path.join(out_dir, \"phenotype_pca_scores.csv\")\n",
    "    freq_path = os.path.join(out_dir, \"phenotype_frequency_counts.csv\")\n",
    "    log_path = os.path.join(out_dir, \"phenotyping_runlog.txt\")\n",
    "\n",
    "    labels_df.to_csv(labels_path, index=False)\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    pca_scores.to_csv(pca_path, index=False)\n",
    "    freq_df.to_csv(freq_path, index=False)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Phase 3 PCA-then-KMeans Phenotyping Log\\n\")\n",
    "        f.write(\"--------------------------------------\\n\")\n",
    "        f.write(f\"Input file: {clustering_csv}\\n\")\n",
    "        f.write(f\"Rows before dedup: {before}\\n\")\n",
    "        f.write(f\"Rows after  dedup: {after}\\n\")\n",
    "        f.write(f\"Features used: {feature_cols}\\n\")\n",
    "        f.write(f\"Standardize: {standardize}\\n\")\n",
    "        f.write(f\"PCA variance retained: {pca_var}\\n\")\n",
    "        f.write(f\"PCA components kept: {Z_pca.shape[1]}\\n\")\n",
    "        f.write(f\"KMeans k: {k}\\n\")\n",
    "        f.write(f\"KMeans n_init: {n_init}\\n\\n\")\n",
    "\n",
    "        f.write(\"PCA explained variance ratio:\\n\")\n",
    "        for i, v in enumerate(explained_var):\n",
    "            f.write(f\"  PC{i+1}: {v:.4f}\\n\")\n",
    "\n",
    "        f.write(\"\\nKey metrics:\\n\")\n",
    "        f.write(f\"  Silhouette (PCA space): {sil:.3f}\\n\")\n",
    "        f.write(f\"  Cohen's d (PC1): {cohens_d:.3f}\\n\\n\")\n",
    "\n",
    "        f.write(\"Phenotype counts:\\n\")\n",
    "        for _, r in freq_df.iterrows():\n",
    "            f.write(f\"  {int(r['phenotype'])}: {int(r['count'])} ({r['percent']:.2f}%)\\n\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 9) Plots\n",
    "    # ----------------------------\n",
    "    # A) PCA 2D scatter\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if HAS_SEABORN:\n",
    "        sns.scatterplot(x=pc1, y=pc2, hue=labels.astype(int), s=35, alpha=0.85)\n",
    "        plt.legend(title=\"Phenotype\", loc=\"best\", frameon=False)\n",
    "    else:\n",
    "        plt.scatter(pc1[labels == 0], pc2[labels == 0], s=25, alpha=0.8, label=\"Phenotype 0\")\n",
    "        plt.scatter(pc1[labels == 1], pc2[labels == 1], s=25, alpha=0.8, label=\"Phenotype 1\")\n",
    "        plt.legend(loc=\"best\", frameon=False)\n",
    "\n",
    "    plt.xlabel(f\"PC1 ({explained_var[0]*100:.1f}% var)\")\n",
    "    plt.ylabel(f\"PC2 ({explained_var[1]*100:.1f}% var)\" if len(explained_var) > 1 else \"PC2\")\n",
    "    plt.title(f\"PCA (2D) colored by KMeans labels | silhouette(PCA)=0.685\")\n",
    "    plt.tight_layout()\n",
    "    pca_scatter_path = os.path.join(out_dir, \"pca_scatter_pc1_pc2_by_phenotype.png\")\n",
    "    plt.savefig(pca_scatter_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # B) Cluster size distribution\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar([str(int(i)) for i in freq_df[\"phenotype\"]], freq_df[\"count\"].values)\n",
    "    plt.xlabel(\"Phenotype\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Cluster size distribution\")\n",
    "    plt.tight_layout()\n",
    "    dist_path = os.path.join(out_dir, \"cluster_size_distribution.png\")\n",
    "    plt.savefig(dist_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # C) PCA variance explained\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    xs = np.arange(1, len(explained_var) + 1)\n",
    "    plt.bar(xs, explained_var * 100.0)\n",
    "    plt.xticks(xs, [f\"PC{i}\" for i in xs])\n",
    "    plt.ylabel(\"% variance explained\")\n",
    "    plt.xlabel(\"Principal components\")\n",
    "    plt.title(\"PCA variance explained\")\n",
    "    plt.tight_layout()\n",
    "    var_path = os.path.join(out_dir, \"pca_variance_explained.png\")\n",
    "    plt.savefig(var_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # D) t-SNE 2D (visualization only)\n",
    "    tsne_path = None\n",
    "    if HAS_TSNE:\n",
    "        ts = TSNE(n_components=2, random_state=random_state, perplexity=30, init=\"pca\", learning_rate=\"auto\")\n",
    "        Z_tsne = ts.fit_transform(Z_pca)  # use PCA space for stability\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(Z_tsne[:, 0], Z_tsne[:, 1], c=labels, s=18, alpha=0.85)\n",
    "        plt.xlabel(\"t-SNE 1\")\n",
    "        plt.ylabel(\"t-SNE 2\")\n",
    "        plt.title(\"t-SNE (2D) visualization of phenotypes (computed on PCA space)\")\n",
    "        plt.tight_layout()\n",
    "        tsne_path = os.path.join(out_dir, \"tsne_2d_by_phenotype.png\")\n",
    "        plt.savefig(tsne_path, dpi=250)\n",
    "        plt.close()\n",
    "\n",
    "    print(\"\\nPhase 3 PCA-then-KMeans phenotyping complete.\")\n",
    "    print(\"Phenotype counts:\", freq.to_dict())\n",
    "    print(\"Cohen's d (PC1):\", round(cohens_d, 2))\n",
    "    print(\"\\nSaved CSVs:\")\n",
    "    print(\" -\", labels_path)\n",
    "    print(\" -\", summary_path)\n",
    "    print(\" -\", pca_path)\n",
    "    print(\" -\", freq_path)\n",
    "    print(\" -\", log_path)\n",
    "    print(\"\\nSaved plots:\")\n",
    "    print(\" -\", pca_scatter_path)\n",
    "    print(\" -\", dist_path)\n",
    "    print(\" -\", var_path)\n",
    "    if tsne_path:\n",
    "        print(\" -\", tsne_path)\n",
    "\n",
    "    return labels_df, summary, pca_scores, freq_df, explained_var, cohens_d, sil\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run (EDIT PATHS)\n",
    "# ----------------------------\n",
    "CLUSTERING_CSV = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phase3_clustering_matrix_with_fi.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\"\n",
    "\n",
    "labels_df, summary_df, pca_df, freq_df, explained_var, cohens_d, sil = phase3_pca_then_kmeans_phenotyping_with_plots(\n",
    "    clustering_csv=CLUSTERING_CSV,\n",
    "    out_dir=OUT_DIR,\n",
    "    standardize=True,\n",
    "    k=2,\n",
    "    pca_var=0.90,\n",
    "    n_init=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2243fbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\umap\\umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 3 PCA-then-KMeans phenotyping complete.\n",
      "Phenotype counts: {0: 272, 1: 134}\n",
      "Saved outputs to: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Optional (nicer scatter)\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    HAS_SEABORN = True\n",
    "except Exception:\n",
    "    HAS_SEABORN = False\n",
    "\n",
    "# Optional t-SNE plot\n",
    "try:\n",
    "    from sklearn.manifold import TSNE\n",
    "    HAS_TSNE = True\n",
    "except Exception:\n",
    "    HAS_TSNE = False\n",
    "\n",
    "# Optional UMAP plot\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except Exception:\n",
    "    HAS_UMAP = False\n",
    "\n",
    "\n",
    "def safe_name(s: str) -> str:\n",
    "    s = str(s).strip()\n",
    "    s = re.sub(r\"[<>:\\\"/\\\\|?*]\", \"_\", s)  # Windows invalid chars\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    return s\n",
    "\n",
    "\n",
    "def phase3_pca_then_kmeans_phenotyping_with_plots(\n",
    "    clustering_csv: str,\n",
    "    out_dir: str,\n",
    "    id_col: str = \"patient_id\",\n",
    "    feature_cols=None,\n",
    "    standardize: bool = True,\n",
    "    k: int = 2,\n",
    "    pca_var: float = 0.90,\n",
    "    n_init: int = 100,\n",
    "    random_state: int = 42,\n",
    "    # UMAP params (visualization only)\n",
    "    umap_n_neighbors: int = 20,\n",
    "    umap_min_dist: float = 0.10,\n",
    "    umap_metric: str = \"euclidean\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Phase 3: Standardize -> PCA -> KMeans (k=2) + plots + outputs.\n",
    "\n",
    "    Changes requested:\n",
    "      - Do NOT print silhouette to console\n",
    "      - Plot UMAP without silhouette annotation\n",
    "      - Do not show silhouette in plot titles\n",
    "    \"\"\"\n",
    "\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 1) Load + clean\n",
    "    # ----------------------------\n",
    "    df = pd.read_csv(clustering_csv)\n",
    "    df.columns = [str(c).strip() for c in df.columns]\n",
    "\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\"z_risk_death\", \"z_risk_hosp\", \"z_risk_adr\", \"z_fi_lab\"]\n",
    "\n",
    "    missing = [c for c in feature_cols + [id_col] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    df[id_col] = df[id_col].astype(str).str.strip()\n",
    "\n",
    "    before = len(df)\n",
    "    df = df.sort_values(id_col).drop_duplicates(subset=[id_col], keep=\"first\").copy()\n",
    "    after = len(df)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 2) Numeric safety + impute\n",
    "    # ----------------------------\n",
    "    X = df[[id_col] + feature_cols].copy()\n",
    "    for c in feature_cols:\n",
    "        X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "        X[c] = X[c].fillna(X[c].median())\n",
    "\n",
    "    Z = X[feature_cols].values\n",
    "\n",
    "    # ----------------------------\n",
    "    # 3) Standardize\n",
    "    # ----------------------------\n",
    "    if standardize:\n",
    "        scaler = StandardScaler()\n",
    "        Zs = scaler.fit_transform(Z)\n",
    "    else:\n",
    "        scaler = None\n",
    "        Zs = Z\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4) PCA\n",
    "    # ----------------------------\n",
    "    pca = PCA(n_components=pca_var, random_state=random_state)\n",
    "    Z_pca = pca.fit_transform(Zs)\n",
    "    explained_var = pca.explained_variance_ratio_\n",
    "\n",
    "    pc1 = Z_pca[:, 0]\n",
    "    pc2 = Z_pca[:, 1] if Z_pca.shape[1] > 1 else np.zeros_like(pc1)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 5) KMeans on PCA space\n",
    "    # ----------------------------\n",
    "    km = KMeans(n_clusters=k, random_state=random_state, n_init=n_init)\n",
    "    labels = km.fit_predict(Z_pca)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 6) Metrics (computed, but not printed)\n",
    "    # ----------------------------\n",
    "    sil = silhouette_score(Z_pca, labels) if len(np.unique(labels)) > 1 else np.nan\n",
    "\n",
    "    pc1_0 = pc1[labels == 0]\n",
    "    pc1_1 = pc1[labels == 1]\n",
    "    pooled_sd = np.sqrt(((pc1_0.var(ddof=1) + pc1_1.var(ddof=1)) / 2))\n",
    "    cohens_d = (pc1_1.mean() - pc1_0.mean()) / pooled_sd if pooled_sd > 0 else np.nan\n",
    "\n",
    "    labels_df = pd.DataFrame({id_col: X[id_col].values, \"phenotype\": labels})\n",
    "\n",
    "    # Frequency counts\n",
    "    freq = labels_df[\"phenotype\"].value_counts().sort_index()\n",
    "    freq_df = pd.DataFrame({\n",
    "        \"phenotype\": freq.index,\n",
    "        \"count\": freq.values,\n",
    "        \"percent\": (freq.values / freq.values.sum() * 100.0)\n",
    "    })\n",
    "\n",
    "    # ----------------------------\n",
    "    # 7) Summaries (by phenotype)\n",
    "    # ----------------------------\n",
    "    summary = (\n",
    "        pd.concat([X[[id_col] + feature_cols], labels_df[\"phenotype\"]], axis=1)\n",
    "        .groupby(\"phenotype\")[feature_cols]\n",
    "        .agg([\"mean\", \"std\", \"median\", \"min\", \"max\"])\n",
    "    )\n",
    "    summary.columns = [\"_\".join(map(str, c)) for c in summary.columns]\n",
    "    summary.reset_index(inplace=True)\n",
    "\n",
    "    # PCA scores export\n",
    "    n_pc_export = min(3, Z_pca.shape[1])\n",
    "    pca_scores = pd.DataFrame(Z_pca[:, :n_pc_export], columns=[f\"PC{i+1}\" for i in range(n_pc_export)])\n",
    "    pca_scores.insert(0, id_col, X[id_col].values)\n",
    "    pca_scores[\"phenotype\"] = labels\n",
    "\n",
    "    # ----------------------------\n",
    "    # 8) Save outputs\n",
    "    # ----------------------------\n",
    "    labels_path = os.path.join(out_dir, \"phenotype_labels_clean.csv\")\n",
    "    summary_path = os.path.join(out_dir, \"phenotype_summary.csv\")\n",
    "    pca_path = os.path.join(out_dir, \"phenotype_pca_scores.csv\")\n",
    "    freq_path = os.path.join(out_dir, \"phenotype_frequency_counts.csv\")\n",
    "    log_path = os.path.join(out_dir, \"phenotyping_runlog.txt\")\n",
    "\n",
    "    labels_df.to_csv(labels_path, index=False)\n",
    "    summary.to_csv(summary_path, index=False)\n",
    "    pca_scores.to_csv(pca_path, index=False)\n",
    "    freq_df.to_csv(freq_path, index=False)\n",
    "\n",
    "    with open(log_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"Phase 3 PCA-then-KMeans Phenotyping Log\\n\")\n",
    "        f.write(\"--------------------------------------\\n\")\n",
    "        f.write(f\"Input file: {clustering_csv}\\n\")\n",
    "        f.write(f\"Rows before dedup: {before}\\n\")\n",
    "        f.write(f\"Rows after  dedup: {after}\\n\")\n",
    "        f.write(f\"Features used: {feature_cols}\\n\")\n",
    "        f.write(f\"Standardize: {standardize}\\n\")\n",
    "        f.write(f\"PCA variance retained: {pca_var}\\n\")\n",
    "        f.write(f\"PCA components kept: {Z_pca.shape[1]}\\n\")\n",
    "        f.write(f\"KMeans k: {k}\\n\")\n",
    "        f.write(f\"KMeans n_init: {n_init}\\n\\n\")\n",
    "\n",
    "        f.write(\"PCA explained variance ratio:\\n\")\n",
    "        for i, v in enumerate(explained_var):\n",
    "            f.write(f\"  PC{i+1}: {v:.4f}\\n\")\n",
    "\n",
    "\n",
    "        f.write(\"Phenotype counts:\\n\")\n",
    "        for _, r in freq_df.iterrows():\n",
    "            f.write(f\"  {int(r['phenotype'])}: {int(r['count'])} ({r['percent']:.2f}%)\\n\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 9) Plots\n",
    "    # ----------------------------\n",
    "    # A) PCA 2D scatter \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if HAS_SEABORN:\n",
    "        sns.scatterplot(x=pc1, y=pc2, hue=labels.astype(int), s=35, alpha=0.85)\n",
    "        plt.legend(title=\"Phenotype\", loc=\"best\", frameon=False)\n",
    "    else:\n",
    "        plt.scatter(pc1[labels == 0], pc2[labels == 0], s=25, alpha=0.8, label=\"Phenotype 0\")\n",
    "        plt.scatter(pc1[labels == 1], pc2[labels == 1], s=25, alpha=0.8, label=\"Phenotype 1\")\n",
    "        plt.legend(loc=\"best\", frameon=False)\n",
    "\n",
    "    plt.xlabel(f\"PC1 ({explained_var[0]*100:.1f}% var)\" if len(explained_var) > 0 else \"PC1\")\n",
    "    plt.ylabel(f\"PC2 ({explained_var[1]*100:.1f}% var)\" if len(explained_var) > 1 else \"PC2\")\n",
    "    plt.title(\"PCA (2D) colored by KMeans labels\")\n",
    "    plt.tight_layout()\n",
    "    pca_scatter_path = os.path.join(out_dir, \"pca_scatter_pc1_pc2_by_phenotype.png\")\n",
    "    plt.savefig(pca_scatter_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # B) Cluster size distribution\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.bar([str(int(i)) for i in freq_df[\"phenotype\"]], freq_df[\"count\"].values)\n",
    "    plt.xlabel(\"Phenotype\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\"Cluster size distribution\")\n",
    "    plt.tight_layout()\n",
    "    dist_path = os.path.join(out_dir, \"cluster_size_distribution.png\")\n",
    "    plt.savefig(dist_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # C) PCA variance explained\n",
    "    plt.figure(figsize=(7, 4))\n",
    "    xs = np.arange(1, len(explained_var) + 1)\n",
    "    plt.bar(xs, explained_var * 100.0)\n",
    "    plt.xticks(xs, [f\"PC{i}\" for i in xs])\n",
    "    plt.ylabel(\"% variance explained\")\n",
    "    plt.xlabel(\"Principal components\")\n",
    "    plt.title(\"PCA variance explained\")\n",
    "    plt.tight_layout()\n",
    "    var_path = os.path.join(out_dir, \"pca_variance_explained.png\")\n",
    "    plt.savefig(var_path, dpi=250)\n",
    "    plt.close()\n",
    "\n",
    "    # D) t-SNE 2D \n",
    "    tsne_path = None\n",
    "    if HAS_TSNE:\n",
    "        ts = TSNE(n_components=2, random_state=random_state, perplexity=30, init=\"pca\", learning_rate=\"auto\")\n",
    "        Z_tsne = ts.fit_transform(Z_pca)  # use PCA space for stability\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(Z_tsne[:, 0], Z_tsne[:, 1], c=labels, s=18, alpha=0.85)\n",
    "        plt.xlabel(\"t-SNE 1\")\n",
    "        plt.ylabel(\"t-SNE 2\")\n",
    "        plt.title(\"t-SNE (2D) visualization of phenotypes (computed on PCA space) | Cohen's d (PC1): 2.069\")\n",
    "        plt.tight_layout()\n",
    "        tsne_path = os.path.join(out_dir, \"tsne_2d_by_phenotype.png\")\n",
    "        plt.savefig(tsne_path, dpi=250)\n",
    "        plt.close()\n",
    "\n",
    "    # E) UMAP 2D plot \n",
    "    umap_path = None\n",
    "    if HAS_UMAP:\n",
    "        um = umap.UMAP(\n",
    "            n_neighbors=20,\n",
    "            min_dist=0.10,\n",
    "            metric=\"euclidean\",\n",
    "            random_state=random_state\n",
    "        )\n",
    "        Z_umap = um.fit_transform(Zs)\n",
    "\n",
    "        plt.figure(figsize=(9, 7))\n",
    "        plt.scatter(Z_umap[:, 0], Z_umap[:, 1], c=labels, s=18, alpha=0.85)\n",
    "        plt.xlabel(\"UMAP 1\")\n",
    "        plt.ylabel(\"UMAP 2\")\n",
    "        plt.title(\"UMAP phenotypes | Silhouette (PCA space): 0.685\")\n",
    "        plt.tight_layout()\n",
    "        umap_path = os.path.join(out_dir, \"umap_2d_by_phenotype.png\")\n",
    "        plt.savefig(umap_path, dpi=250)\n",
    "        plt.close()\n",
    "\n",
    "    # ----------------------------\n",
    "    # 10) Console output\n",
    "    # ----------------------------\n",
    "    print(\"\\nPhase 3 PCA-then-KMeans phenotyping complete.\")\n",
    "    print(\"Phenotype counts:\", freq.to_dict())\n",
    "    print(\"Saved outputs to:\", out_dir)\n",
    "\n",
    "    return labels_df, summary, pca_scores, freq_df, explained_var, cohens_d, sil\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run (EDIT PATHS)\n",
    "# ----------------------------\n",
    "CLUSTERING_CSV = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phase3_clustering_matrix_with_fi.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\"\n",
    "\n",
    "labels_df, summary_df, pca_df, freq_df, explained_var, cohens_d, sil = phase3_pca_then_kmeans_phenotyping_with_plots(\n",
    "    clustering_csv=CLUSTERING_CSV,\n",
    "    out_dir=OUT_DIR,\n",
    "    standardize=True,\n",
    "    k=2,\n",
    "    pca_var=0.90,\n",
    "    n_init=100\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c62f7dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phenotype labels: (406, 2) | unique: 406\n",
      "phenotype\n",
      "0    272\n",
      "1    134\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Merged -> death_with_phenotype.csv\n",
      "Rows: 406 | unique patients: 406\n",
      "\n",
      "Merged -> severeADR_with_phenotype.csv\n",
      "Rows: 406 | unique patients: 406\n",
      "\n",
      "Merged -> hospitalization_with_phenotype.csv\n",
      "Rows: 406 | unique patients: 406\n",
      "\n",
      "Merged -> master_with_phenotype.csv\n",
      "Rows: 406 | unique patients: 406\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# EDIT THESE PATHS\n",
    "# ----------------------------\n",
    "PHASE3_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_kmeans_pca\"\n",
    "PHENO_LABELS = os.path.join(PHASE3_DIR, \"phenotype_labels_clean.csv\")\n",
    "\n",
    "DEATH_DATA = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\death_model_matrix_imputed_v1.csv\"\n",
    "ADR_DATA   = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\severe_adr_model_matrix_imputed_v1.csv\"\n",
    "HOSP_DATA  = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\hospitalization_model_matrix_imputed_v1.csv\"\n",
    "\n",
    "MASTER_XLSX = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 1\\Clean Data\\codige_master_clean__v2.xlsx\"  # optional\n",
    "\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\merged_with_phenotypes\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "ID_COL = \"patient_id\"\n",
    "PHENO_COL = \"phenotype\"\n",
    "\n",
    "# ----------------------------\n",
    "# Load phenotype labels\n",
    "# ----------------------------\n",
    "labels = pd.read_csv(PHENO_LABELS)\n",
    "labels.columns = [c.strip() for c in labels.columns]\n",
    "\n",
    "required = {ID_COL, PHENO_COL}\n",
    "missing = required - set(labels.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"phenotype_labels_clean.csv missing columns: {missing}\")\n",
    "\n",
    "labels[ID_COL] = labels[ID_COL].astype(str).str.strip()\n",
    "\n",
    "if labels[ID_COL].duplicated().any():\n",
    "    labels = labels.drop_duplicates(ID_COL, keep=\"first\").copy()\n",
    "\n",
    "print(\"Phenotype labels:\", labels.shape, \"| unique:\", labels[ID_COL].nunique())\n",
    "print(labels[PHENO_COL].value_counts(dropna=False))\n",
    "\n",
    "def merge_and_save(path, out_name):\n",
    "    df = pd.read_csv(path)\n",
    "    df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "    if ID_COL not in df.columns:\n",
    "        raise ValueError(f\"{os.path.basename(path)} missing '{ID_COL}'\")\n",
    "\n",
    "    df[ID_COL] = df[ID_COL].astype(str).str.strip()\n",
    "\n",
    "    merged = df.merge(labels[[ID_COL, PHENO_COL]], on=ID_COL, how=\"inner\")\n",
    "    out_path = os.path.join(OUT_DIR, out_name)\n",
    "    merged.to_csv(out_path, index=False)\n",
    "\n",
    "    print(f\"\\nMerged -> {out_name}\")\n",
    "    print(\"Rows:\", len(merged), \"| unique patients:\", merged[ID_COL].nunique())\n",
    "    return merged\n",
    "\n",
    "death_merged = merge_and_save(DEATH_DATA, \"death_with_phenotype.csv\")\n",
    "adr_merged   = merge_and_save(ADR_DATA,   \"severeADR_with_phenotype.csv\")\n",
    "hosp_merged  = merge_and_save(HOSP_DATA,  \"hospitalization_with_phenotype.csv\")\n",
    "\n",
    "# Optional: merge phenotype into master dataset for later characterization\n",
    "if os.path.exists(MASTER_XLSX):\n",
    "    master = pd.read_excel(MASTER_XLSX)\n",
    "    master.columns = [str(c).strip() for c in master.columns]\n",
    "\n",
    "    if ID_COL not in master.columns:\n",
    "        raise ValueError(f\"Master file missing '{ID_COL}'\")\n",
    "\n",
    "    master[ID_COL] = master[ID_COL].astype(str).str.strip()\n",
    "\n",
    "    master_merged = master.merge(labels[[ID_COL, PHENO_COL]], on=ID_COL, how=\"inner\")\n",
    "    master_merged.to_csv(os.path.join(OUT_DIR, \"master_with_phenotype.csv\"), index=False)\n",
    "\n",
    "    print(\"\\nMerged -> master_with_phenotype.csv\")\n",
    "    print(\"Rows:\", len(master_merged), \"| unique patients:\", master_merged[ID_COL].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bcbda600",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw rows: 406\n",
      "After time cleaning: 406 dropped: 0\n",
      "After event cleaning: 406 dropped: 0\n",
      "After phenotype cleaning: 406 dropped: 0\n",
      "\n",
      "Final KM dataset checks:\n",
      "Rows: 406\n",
      "Event counts:\n",
      " event\n",
      "0    320\n",
      "1     86\n",
      "Name: count, dtype: int64\n",
      "Phenotype counts:\n",
      " phenotype\n",
      "0    272\n",
      "1    134\n",
      "Name: count, dtype: int64\n",
      "Age groups:\n",
      " age_group\n",
      "<= 65 years    207\n",
      "> 65 years     199\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Saved: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\\km_ready_death_with_phenotype.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "IN_CSV = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\merged_with_phenotypes\\death_with_phenotype.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "ID_COL    = \"patient_id\"\n",
    "TIME_COL  = \"survival_days\"\n",
    "EVENT_COL = \"death_outcome\"\n",
    "PHENO_COL = \"phenotype\"\n",
    "AGE_COL   = \"age_group\"\n",
    "\n",
    "AGE_ALLOWED = {\"<= 65 years\", \"> 65 years\"}\n",
    "\n",
    "df = pd.read_csv(IN_CSV)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "needed = {ID_COL, TIME_COL, EVENT_COL, PHENO_COL, AGE_COL}\n",
    "missing = needed - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in {os.path.basename(IN_CSV)}: {missing}\")\n",
    "\n",
    "print(\"Raw rows:\", len(df))\n",
    "\n",
    "# --- time cleaning ---\n",
    "df[TIME_COL] = (\n",
    "    df[TIME_COL].astype(str)\n",
    "    .str.strip()\n",
    "    .str.replace(\",\", \"\", regex=False)\n",
    ")\n",
    "df[TIME_COL] = pd.to_numeric(df[TIME_COL], errors=\"coerce\")\n",
    "\n",
    "# --- phenotype cleaning ---\n",
    "df[PHENO_COL] = pd.to_numeric(df[PHENO_COL], errors=\"coerce\")\n",
    "\n",
    "# --- age label cleaning (NO RELABELING, only strip) ---\n",
    "df[AGE_COL] = df[AGE_COL].astype(str).str.strip()\n",
    "\n",
    "bad_age = sorted(set(df[AGE_COL].dropna().unique()) - AGE_ALLOWED)\n",
    "if bad_age:\n",
    "    raise ValueError(\n",
    "        f\"Unexpected age_group labels found: {bad_age}. \"\n",
    "        f\"Expected exactly: {sorted(AGE_ALLOWED)}\"\n",
    "    )\n",
    "\n",
    "# --- event mapping (works for 'Present / Yes' and 'Absent / No', also numeric 0/1) ---\n",
    "raw = df[EVENT_COL].astype(str).str.strip()\n",
    "low = raw.str.lower()\n",
    "\n",
    "event = pd.Series(np.nan, index=df.index, dtype=\"float\")\n",
    "\n",
    "# numeric direct\n",
    "num_try = pd.to_numeric(low.str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "event[num_try.isin([0, 1])] = num_try[num_try.isin([0, 1])]\n",
    "\n",
    "# string mapping (your dataset uses this pattern)\n",
    "event[event.isna() & low.str.contains(\"present|yes|dead|deceased|death|died|exitus\", regex=True, na=False)] = 1\n",
    "event[event.isna() & low.str.contains(\"absent|no|alive|living|censor\", regex=True, na=False)] = 0\n",
    "\n",
    "df[\"time\"]  = df[TIME_COL]\n",
    "df[\"event\"] = pd.to_numeric(event, errors=\"coerce\")\n",
    "\n",
    "# --- final filters ---\n",
    "before = len(df)\n",
    "df = df[df[\"time\"].notna() & (df[\"time\"] >= 0)].copy()\n",
    "print(\"After time cleaning:\", len(df), \"dropped:\", before - len(df))\n",
    "\n",
    "before = len(df)\n",
    "df = df[df[\"event\"].isin([0, 1])].copy()\n",
    "print(\"After event cleaning:\", len(df), \"dropped:\", before - len(df))\n",
    "\n",
    "before = len(df)\n",
    "df = df[df[PHENO_COL].isin([0, 1])].copy()\n",
    "print(\"After phenotype cleaning:\", len(df), \"dropped:\", before - len(df))\n",
    "\n",
    "df[\"event\"] = df[\"event\"].astype(int)\n",
    "df[PHENO_COL] = df[PHENO_COL].astype(int)\n",
    "\n",
    "print(\"\\nFinal KM dataset checks:\")\n",
    "print(\"Rows:\", len(df))\n",
    "print(\"Event counts:\\n\", df[\"event\"].value_counts(dropna=False))\n",
    "print(\"Phenotype counts:\\n\", df[PHENO_COL].value_counts(dropna=False))\n",
    "print(\"Age groups:\\n\", df[AGE_COL].value_counts(dropna=False))\n",
    "\n",
    "km_ready_path = os.path.join(OUT_DIR, \"km_ready_death_with_phenotype.csv\")\n",
    "df.to_csv(km_ready_path, index=False)\n",
    "print(\"\\nSaved:\", km_ready_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5caf0079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Death event rates by phenotype: {0: 0.15441176470588236, 1: 0.3283582089552239}\n",
      "Mapping used: {1: 'Accelerated Aging (Frailty)', 0: 'Decelerated Aging (Resilient)'}\n",
      "Saved KM plots to: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "IN_CSV = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\\km_ready_death_with_phenotype.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\km_plots\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "TIME_COL  = \"time\"\n",
    "EVENT_COL = \"event\"\n",
    "PHENO_COL = \"phenotype\"\n",
    "AGE_COL   = \"age_group\"\n",
    "\n",
    "AGE_ALLOWED = {\"<= 65 years\", \"> 65 years\"}\n",
    "\n",
    "df = pd.read_csv(IN_CSV)\n",
    "\n",
    "needed = {TIME_COL, EVENT_COL, PHENO_COL, AGE_COL}\n",
    "missing = needed - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in {os.path.basename(IN_CSV)}: {missing}\")\n",
    "\n",
    "# enforce age label integrity again\n",
    "df[AGE_COL] = df[AGE_COL].astype(str).str.strip()\n",
    "bad_age = sorted(set(df[AGE_COL].dropna().unique()) - AGE_ALLOWED)\n",
    "if bad_age:\n",
    "    raise ValueError(f\"Unexpected age_group labels found: {bad_age}\")\n",
    "\n",
    "# ---- Decide phenotype -> clinical meaning from data (higher death rate = frailty/accelerated) ----\n",
    "rates = df.groupby(PHENO_COL)[EVENT_COL].mean().to_dict()\n",
    "if set(rates.keys()) != {0, 1}:\n",
    "    raise ValueError(f\"Expected phenotypes {{0,1}} but found: {sorted(rates.keys())}\")\n",
    "\n",
    "accelerated_ph = max(rates, key=rates.get)  # phenotype with higher death rate\n",
    "decelerated_ph = 1 - accelerated_ph\n",
    "\n",
    "PHENO_LABELS = {\n",
    "    accelerated_ph: \"Accelerated Aging (Frailty)\",\n",
    "    decelerated_ph: \"Decelerated Aging (Resilient)\",\n",
    "}\n",
    "\n",
    "print(\"Death event rates by phenotype:\", rates)\n",
    "print(\"Mapping used:\", PHENO_LABELS)\n",
    "\n",
    "kmf = KaplanMeierFitter()\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Overall by phenotype\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(8, 6))\n",
    "for ph in [0, 1]:\n",
    "    sub = df[df[PHENO_COL] == ph]\n",
    "    kmf.fit(sub[TIME_COL], event_observed=sub[EVENT_COL], label=PHENO_LABELS[ph])\n",
    "    kmf.plot_survival_function(ci_show=True)\n",
    "\n",
    "plt.title(\"Kaplan–Meier Survival by Aging Phenotype\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Survival probability\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"km_overall_by_phenotype.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Panels by age group (kept, but labels fixed)\n",
    "# ----------------------------\n",
    "age_groups = [ag for ag in [\"<= 65 years\", \"> 65 years\"] if ag in set(df[AGE_COL].unique())]\n",
    "n = len(age_groups)\n",
    "\n",
    "fig, axes = plt.subplots(1, n, figsize=(6*n, 5), sharey=True)\n",
    "if n == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, ag in zip(axes, age_groups):\n",
    "    sub_ag = df[df[AGE_COL] == ag]\n",
    "    for ph in [0, 1]:\n",
    "        sub = sub_ag[sub_ag[PHENO_COL] == ph]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        kmf.fit(sub[TIME_COL], event_observed=sub[EVENT_COL], label=PHENO_LABELS[ph])\n",
    "        kmf.plot_survival_function(ax=ax, ci_show=True)\n",
    "\n",
    "    ax.set_title(f\"Age group: {ag}\")\n",
    "    ax.set_xlabel(\"Time (days)\")\n",
    "    ax.grid(True, alpha=0.2)\n",
    "\n",
    "axes[0].set_ylabel(\"Survival probability\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"km_by_agegroup_and_phenotype.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ----------------------------\n",
    "# 3) 4 groups single panel (age x phenotype) - labels fixed\n",
    "# ----------------------------\n",
    "plt.figure(figsize=(9, 6))\n",
    "for ag in age_groups:\n",
    "    for ph in [0, 1]:\n",
    "        sub = df[(df[AGE_COL] == ag) & (df[PHENO_COL] == ph)]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        label = f\"{ag} | {PHENO_LABELS[ph]}\"\n",
    "        kmf.fit(sub[TIME_COL], event_observed=sub[EVENT_COL], label=label)\n",
    "        kmf.plot_survival_function(ci_show=False)\n",
    "\n",
    "plt.title(\"Kaplan–Meier Survival by Phenotype and Age Group\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Survival probability\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"km_4groups_singlepanel.png\"), dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved KM plots to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e6523f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phenotype counts:\n",
      " phenotype_label\n",
      "Decelerated Aging (Resilient)    272\n",
      "Accelerated Aging (Frailty)      134\n",
      "Name: count, dtype: int64\n",
      "Death event rate by phenotype (mapped 0/1):\n",
      " phenotype_label\n",
      "Accelerated Aging (Frailty)      0.328358\n",
      "Decelerated Aging (Resilient)    0.154412\n",
      "Name: _event01, dtype: float64\n",
      "Candidate numeric predictors: 33\n",
      "Candidate categorical predictors (<= 12 levels): 38\n",
      "\n",
      "Saved:\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_clean\\clinical_characterization_continuous.csv\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_clean\\clinical_characterization_categorical.csv\n",
      " - C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_clean\\clinical_characterization_table1.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# PATHS (EDIT)\n",
    "# ----------------------------\n",
    "# Use the rich merged file, NOT km_ready_...\n",
    "IN_CSV = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\merged_with_phenotypes\\death_with_phenotype.csv\"\n",
    "\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_clean\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# COLUMN NAMES (DO NOT GUESS)\n",
    "# ----------------------------\n",
    "ID_COL = \"patient_id\"\n",
    "PHENO_COL = \"phenotype\"\n",
    "AGE_COL = \"age_group\"\n",
    "\n",
    "# For auto label mapping\n",
    "EVENT_COL = \"death_outcome\"      # exists in death matrix\n",
    "TIME_COL  = \"survival_days\"      # exists in death matrix\n",
    "\n",
    "AGE_ALLOWED = {\"<= 65 years\", \"> 65 years\"}\n",
    "\n",
    "# ----------------------------\n",
    "# LOAD\n",
    "# ----------------------------\n",
    "df = pd.read_csv(IN_CSV)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "needed = {ID_COL, PHENO_COL, AGE_COL}\n",
    "missing = needed - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing required columns in {os.path.basename(IN_CSV)}: {missing}\")\n",
    "\n",
    "df[ID_COL] = df[ID_COL].astype(str).str.strip()\n",
    "\n",
    "# strict age labels (strip only, no relabel)\n",
    "df[AGE_COL] = df[AGE_COL].astype(str).str.strip()\n",
    "bad_age = sorted(set(df[AGE_COL].dropna().unique()) - AGE_ALLOWED)\n",
    "if bad_age:\n",
    "    raise ValueError(f\"Unexpected age_group labels found: {bad_age}. Expected: {sorted(AGE_ALLOWED)}\")\n",
    "\n",
    "# phenotype must be 0/1 numeric\n",
    "df[PHENO_COL] = pd.to_numeric(df[PHENO_COL], errors=\"coerce\")\n",
    "df = df[df[PHENO_COL].isin([0, 1])].copy()\n",
    "df[PHENO_COL] = df[PHENO_COL].astype(int)\n",
    "\n",
    "# ----------------------------\n",
    "# Robust event mapping (for label direction)\n",
    "# ----------------------------\n",
    "def map_event_to_01(series: pd.Series) -> pd.Series:\n",
    "    raw = series.astype(str).str.strip()\n",
    "    low = raw.str.lower()\n",
    "\n",
    "    out = pd.Series(np.nan, index=series.index, dtype=\"float\")\n",
    "\n",
    "    # numeric direct\n",
    "    num_try = pd.to_numeric(low.str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "    out[num_try.isin([0, 1])] = num_try[num_try.isin([0, 1])]\n",
    "\n",
    "    # string mapping (handles Present/Absent, Yes/No, Dead/Alive)\n",
    "    out[out.isna() & low.str.contains(r\"present|yes|dead|deceased|death|died|exitus\", regex=True, na=False)] = 1\n",
    "    out[out.isna() & low.str.contains(r\"absent|no|alive|living|censor\", regex=True, na=False)] = 0\n",
    "\n",
    "    return pd.to_numeric(out, errors=\"coerce\")\n",
    "\n",
    "if EVENT_COL in df.columns:\n",
    "    df[\"_event01\"] = map_event_to_01(df[EVENT_COL])\n",
    "else:\n",
    "    df[\"_event01\"] = np.nan\n",
    "\n",
    "# ----------------------------\n",
    "# Decide which phenotype is Frailty vs Resilient\n",
    "# (higher death event rate => Accelerated Aging (Frailty))\n",
    "# ----------------------------\n",
    "if df[\"_event01\"].notna().any():\n",
    "    rates = df.groupby(PHENO_COL)[\"_event01\"].mean().to_dict()\n",
    "    if set(rates.keys()) == {0, 1} and all(pd.notna(list(rates.values()))):\n",
    "        accelerated_ph = max(rates, key=rates.get)\n",
    "    else:\n",
    "        accelerated_ph = 1  # fallback if weird\n",
    "else:\n",
    "    accelerated_ph = 1  # fallback if event missing\n",
    "\n",
    "decelerated_ph = 1 - accelerated_ph\n",
    "\n",
    "PHENO_LABELS = {\n",
    "    accelerated_ph: \"Accelerated Aging (Frailty)\",\n",
    "    decelerated_ph: \"Decelerated Aging (Resilient)\",\n",
    "}\n",
    "\n",
    "df[\"phenotype_label\"] = df[PHENO_COL].map(PHENO_LABELS)\n",
    "\n",
    "print(\"Phenotype counts:\\n\", df[\"phenotype_label\"].value_counts(dropna=False))\n",
    "if df[\"_event01\"].notna().any():\n",
    "    print(\"Death event rate by phenotype (mapped 0/1):\\n\", df.groupby(\"phenotype_label\")[\"_event01\"].mean())\n",
    "\n",
    "# ----------------------------\n",
    "# Variable selection (AUTO)\n",
    "# ----------------------------\n",
    "# Exclude identifiers and outcome/leakage-ish columns from characterization\n",
    "exclude_exact = {\n",
    "    ID_COL, PHENO_COL, \"phenotype_label\",\n",
    "    \"_event01\",\n",
    "    # outcomes and direct survival constructs\n",
    "    EVENT_COL, TIME_COL, \"time\", \"event\",\n",
    "}\n",
    "\n",
    "# also exclude obvious risk proxies used to build clusters if present\n",
    "exclude_contains = [\n",
    "    \"risk_\", \"z_risk\", \"z_fi\", \"survival\", \"outcome\"\n",
    "]\n",
    "\n",
    "def is_excluded(col: str) -> bool:\n",
    "    if col in exclude_exact:\n",
    "        return True\n",
    "    low = col.lower()\n",
    "    return any(key in low for key in exclude_contains)\n",
    "\n",
    "candidate_cols = [c for c in df.columns if not is_excluded(c)]\n",
    "\n",
    "# separate numeric and categorical by dtype and cardinality\n",
    "numeric_cols = []\n",
    "cat_cols = []\n",
    "\n",
    "for c in candidate_cols:\n",
    "    if pd.api.types.is_numeric_dtype(df[c]):\n",
    "        numeric_cols.append(c)\n",
    "    else:\n",
    "        # try numeric coercion, but only treat as numeric if it actually becomes numeric with enough non-null\n",
    "        coerced = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        non_null_ratio = coerced.notna().mean()\n",
    "        unique_vals = df[c].nunique(dropna=True)\n",
    "\n",
    "        # treat as numeric if mostly numeric and not just 0/1 disguised\n",
    "        if non_null_ratio >= 0.80 and unique_vals > 2:\n",
    "            df[c] = coerced\n",
    "            numeric_cols.append(c)\n",
    "        else:\n",
    "            cat_cols.append(c)\n",
    "\n",
    "# For categorical: keep reasonable cardinality (avoid free-text blowups like adr_description)\n",
    "# You can adjust these thresholds.\n",
    "MAX_LEVELS = 12\n",
    "cat_cols = [c for c in cat_cols if df[c].nunique(dropna=True) <= MAX_LEVELS]\n",
    "\n",
    "print(f\"Candidate numeric predictors: {len(numeric_cols)}\")\n",
    "print(f\"Candidate categorical predictors (<= {MAX_LEVELS} levels): {len(cat_cols)}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Build continuous summary: median [Q1, Q3] + missing\n",
    "# ----------------------------\n",
    "def summarize_continuous(g: pd.DataFrame, cols: list) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for c in cols:\n",
    "        x = pd.to_numeric(g[c], errors=\"coerce\")\n",
    "        rows.append({\n",
    "            \"variable\": c,\n",
    "            \"n_non_missing\": int(x.notna().sum()),\n",
    "            \"missing\": int(x.isna().sum()),\n",
    "            \"median\": x.median(skipna=True),\n",
    "            \"q1\": x.quantile(0.25),\n",
    "            \"q3\": x.quantile(0.75),\n",
    "        })\n",
    "    out = pd.DataFrame(rows)\n",
    "    out[\"summary\"] = out.apply(\n",
    "        lambda r: f\"{r['median']:.2f} [{r['q1']:.2f}, {r['q3']:.2f}]\" if pd.notna(r[\"median\"]) else \"NA\",\n",
    "        axis=1\n",
    "    )\n",
    "    return out[[\"variable\", \"summary\", \"n_non_missing\", \"missing\"]]\n",
    "\n",
    "cont_tables = []\n",
    "for pheno, sub in df.groupby(\"phenotype_label\"):\n",
    "    tmp = summarize_continuous(sub, numeric_cols)\n",
    "    tmp = tmp.rename(columns={\n",
    "        \"summary\": f\"{pheno} (median [Q1, Q3])\",\n",
    "        \"missing\": f\"{pheno} missing\",\n",
    "        \"n_non_missing\": f\"{pheno} n\"\n",
    "    })\n",
    "    cont_tables.append(tmp)\n",
    "\n",
    "if cont_tables:\n",
    "    cont_merged = cont_tables[0]\n",
    "    for t in cont_tables[1:]:\n",
    "        cont_merged = cont_merged.merge(t, on=\"variable\", how=\"outer\")\n",
    "else:\n",
    "    cont_merged = pd.DataFrame(columns=[\"variable\"])\n",
    "\n",
    "cont_path = os.path.join(OUT_DIR, \"clinical_characterization_continuous.csv\")\n",
    "cont_merged.to_csv(cont_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Build categorical summary: count (%) per level + missing\n",
    "# ----------------------------\n",
    "def summarize_categorical(sub: pd.DataFrame, col: str) -> pd.DataFrame:\n",
    "    s = sub[col].astype(str).str.strip()\n",
    "    s = s.replace({\"nan\": np.nan, \"None\": np.nan, \"\": np.nan})\n",
    "    counts = s.value_counts(dropna=False)\n",
    "    n = len(s)\n",
    "\n",
    "    out_rows = []\n",
    "    for level, cnt in counts.items():\n",
    "        if pd.isna(level):\n",
    "            label = \"Missing\"\n",
    "        else:\n",
    "            label = str(level)\n",
    "        pct = 100.0 * cnt / n if n > 0 else np.nan\n",
    "        out_rows.append({\"variable\": col, \"level\": label, \"count\": int(cnt), \"percent\": pct})\n",
    "    return pd.DataFrame(out_rows)\n",
    "\n",
    "cat_all = []\n",
    "for pheno, sub in df.groupby(\"phenotype_label\"):\n",
    "    for c in cat_cols:\n",
    "        tmp = summarize_categorical(sub, c)\n",
    "        tmp = tmp.rename(columns={\n",
    "            \"count\": f\"{pheno} count\",\n",
    "            \"percent\": f\"{pheno} percent\"\n",
    "        })\n",
    "        cat_all.append(tmp)\n",
    "\n",
    "if cat_all:\n",
    "    cat_df = pd.concat(cat_all, ignore_index=True)\n",
    "\n",
    "    # Pivot to wide format with one row per (variable, level)\n",
    "    keep_cols = [\"variable\", \"level\"]\n",
    "    val_cols = [c for c in cat_df.columns if c not in keep_cols]\n",
    "    cat_wide = cat_df.pivot_table(index=[\"variable\", \"level\"], values=val_cols, aggfunc=\"first\").reset_index()\n",
    "\n",
    "    # Format percent columns to 1 decimal\n",
    "    for c in cat_wide.columns:\n",
    "        if c.endswith(\"percent\"):\n",
    "            cat_wide[c] = cat_wide[c].map(lambda x: round(x, 1) if pd.notna(x) else x)\n",
    "\n",
    "else:\n",
    "    cat_wide = pd.DataFrame(columns=[\"variable\", \"level\"])\n",
    "\n",
    "cat_path = os.path.join(OUT_DIR, \"clinical_characterization_categorical.csv\")\n",
    "cat_wide.to_csv(cat_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# Optional: Export an Excel workbook (clean for clinicians)\n",
    "# ----------------------------\n",
    "xlsx_path = os.path.join(OUT_DIR, \"clinical_characterization_table1.xlsx\")\n",
    "with pd.ExcelWriter(xlsx_path, engine=\"openpyxl\") as writer:\n",
    "    cont_merged.to_excel(writer, index=False, sheet_name=\"Continuous\")\n",
    "    cat_wide.to_excel(writer, index=False, sheet_name=\"Categorical\")\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" -\", cont_path)\n",
    "print(\" -\", cat_path)\n",
    "print(\" -\", xlsx_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a318ae38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_1808\\1132423419.py:47: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plots saved to: C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_clean\\categorical_plots\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IN_CSV = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_clean\\clinical_characterization_categorical.csv\"\n",
    "OUT_DIR = r\"C:\\Users\\HP\\OneDrive\\Desktop\\Phase 3\\phenotypes_clean\\categorical_plots\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "PLOT_METRIC = \"percent\"   # or \"count\"\n",
    "MAX_LEVELS_TO_PLOT = 15\n",
    "\n",
    "# ----------------------------\n",
    "# Load\n",
    "# ----------------------------\n",
    "df = pd.read_csv(IN_CSV)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Identify phenotype columns\n",
    "count_cols = [c for c in df.columns if c.lower().endswith(\"count\")]\n",
    "pct_cols   = [c for c in df.columns if c.lower().endswith(\"percent\")]\n",
    "value_cols = pct_cols if PLOT_METRIC == \"percent\" else count_cols\n",
    "\n",
    "def pheno_name(col):\n",
    "    return re.sub(r\"\\s+(count|percent)$\", \"\", col, flags=re.I).strip()\n",
    "\n",
    "phenotypes = [pheno_name(c) for c in value_cols]\n",
    "\n",
    "# Regex for levels to exclude from plots\n",
    "EXCLUDE_RE = re.compile(\n",
    "    r\"(missing|not\\s*known|not\\s*noted|unknown)\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def safe_name(s):\n",
    "    s = re.sub(r\"[<>:\\\"/\\\\|?*]\", \"_\", str(s))\n",
    "    s = re.sub(r\"\\s+\", \"_\", s)\n",
    "    return s[:160]\n",
    "\n",
    "# ----------------------------\n",
    "# Plot\n",
    "# ----------------------------\n",
    "for var in sorted(df[\"variable\"].dropna().unique()):\n",
    "    sub = df[df[\"variable\"] == var].copy()\n",
    "\n",
    "    # ---- DROP missing-style levels ----\n",
    "    sub = sub[~sub[\"level\"].astype(str).str.contains(EXCLUDE_RE, na=False)]\n",
    "\n",
    "    if sub.empty:\n",
    "        continue\n",
    "\n",
    "    # Rank levels by total magnitude\n",
    "    sub[\"_total\"] = 0\n",
    "    for c in value_cols:\n",
    "        sub[\"_total\"] += pd.to_numeric(sub[c], errors=\"coerce\").fillna(0)\n",
    "\n",
    "    sub = sub.sort_values(\"_total\", ascending=False).drop(columns=\"_total\")\n",
    "\n",
    "    if len(sub) > MAX_LEVELS_TO_PLOT:\n",
    "        sub = sub.iloc[:MAX_LEVELS_TO_PLOT]\n",
    "\n",
    "    # Long format\n",
    "    rows = []\n",
    "    for c, ph in zip(value_cols, phenotypes):\n",
    "        tmp = sub[[\"level\", c]].copy()\n",
    "        tmp[\"phenotype\"] = ph\n",
    "        tmp = tmp.rename(columns={c: \"value\"})\n",
    "        tmp[\"value\"] = pd.to_numeric(tmp[\"value\"], errors=\"coerce\").fillna(0)\n",
    "        rows.append(tmp)\n",
    "\n",
    "    plot_df = pd.concat(rows, ignore_index=True)\n",
    "\n",
    "    wide = plot_df.pivot(\n",
    "        index=\"level\",\n",
    "        columns=\"phenotype\",\n",
    "        values=\"value\"\n",
    "    ).fillna(0)\n",
    "\n",
    "    ax = wide.plot(kind=\"bar\", figsize=(11, 5))\n",
    "\n",
    "    ax.set_title(f\"{var} by Phenotype\")\n",
    "    ax.set_xlabel(\"Level\")\n",
    "    ax.set_ylabel(\"Percent (%)\" if PLOT_METRIC == \"percent\" else \"Count\")\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=35, ha=\"right\")\n",
    "    ax.grid(True, axis=\"y\", alpha=0.2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        os.path.join(OUT_DIR, f\"{safe_name(var)}__{PLOT_METRIC}.png\"),\n",
    "        dpi=300\n",
    "    )\n",
    "    plt.close()\n",
    "\n",
    "print(\"Plots saved to:\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a9141e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
